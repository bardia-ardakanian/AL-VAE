{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # Install dependencies\n!pip install pycocotools\n!pip install torchsummary","metadata":{"id":"N555YgUK1n7Z","outputId":"5bb91064-3441-4372-cae5-98d059c38ca2","execution":{"iopub.status.busy":"2023-04-23T06:11:56.601988Z","iopub.execute_input":"2023-04-23T06:11:56.602513Z","iopub.status.idle":"2023-04-23T06:12:38.885393Z","shell.execute_reply.started":"2023-04-23T06:11:56.602465Z","shell.execute_reply":"2023-04-23T06:12:38.883934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nimport torch\nimport random\nfrom typing import Tuple, List\n\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n# Utilities\nimport torchvision\nfrom torchvision.transforms import Compose, Resize, PILToTensor, ToPILImage, ToTensor\nfrom torchsummary import summary\n# pytorch\nfrom torch import nn\nfrom torch.utils.data import DataLoader,random_split, Dataset\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"id":"KF7bLhajiXel","execution":{"iopub.status.busy":"2023-04-23T06:31:12.725814Z","iopub.execute_input":"2023-04-23T06:31:12.726824Z","iopub.status.idle":"2023-04-23T06:31:12.734256Z","shell.execute_reply.started":"2023-04-23T06:31:12.726770Z","shell.execute_reply":"2023-04-23T06:31:12.733159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 0.Preparation\n\n","metadata":{"id":"0JufuosJycku"}},{"cell_type":"code","source":"IMG_WIDTH = 256\nIMG_HEIGHT = 256\nIMG_CHANNELS = 3","metadata":{"id":"TaZJkdiwAIWX","execution":{"iopub.status.busy":"2023-04-23T06:12:41.595160Z","iopub.execute_input":"2023-04-23T06:12:41.595547Z","iopub.status.idle":"2023-04-23T06:12:41.603544Z","shell.execute_reply.started":"2023-04-23T06:12:41.595507Z","shell.execute_reply":"2023-04-23T06:12:41.602415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random seed for reproducible results\ntorch.manual_seed(0)","metadata":{"id":"UoP3E7xuzdjz","outputId":"8c3ed607-84e7-4ec2-e428-e218b56b0e24","execution":{"iopub.status.busy":"2023-04-23T06:12:41.606644Z","iopub.execute_input":"2023-04-23T06:12:41.607804Z","iopub.status.idle":"2023-04-23T06:12:41.618632Z","shell.execute_reply.started":"2023-04-23T06:12:41.607759Z","shell.execute_reply":"2023-04-23T06:12:41.617416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Select Runtime","metadata":{"id":"tRk-Y1tQ2gBP"}},{"cell_type":"code","source":"# Select device (CPU/GPU)\nDEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f\"Running on '{DEVICE}'\")","metadata":{"id":"xMyj4uJH1bIA","outputId":"d18606da-6a13-4f5f-8039-63fb5b2ba26e","execution":{"iopub.status.busy":"2023-04-23T06:12:41.620159Z","iopub.execute_input":"2023-04-23T06:12:41.620735Z","iopub.status.idle":"2023-04-23T06:12:41.686466Z","shell.execute_reply.started":"2023-04-23T06:12:41.620697Z","shell.execute_reply":"2023-04-23T06:12:41.685189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Select dataset source","metadata":{"id":"Lw5xMI2S10c6"}},{"cell_type":"code","source":"# from google.colab import files\n# files.upload()\n\n# # Config Kaggle API\n# !mkdir ~/.kaggle\n# !cp kaggle.json ~/.kaggle/\n# !chmod 600 ~/.kaggle/kaggle.json\n\n# # Download dataset\n# !kaggle datasets download -d bardiaardakanian/mmsample\n# !unzip mmsample.zip -d mmsample\n\n# Copy to Drive\n## !cp -r mmsample /content/drive/MyDrive","metadata":{"id":"5doPJmr-DXv8","execution":{"iopub.status.busy":"2023-04-23T06:12:41.688303Z","iopub.execute_input":"2023-04-23T06:12:41.688755Z","iopub.status.idle":"2023-04-23T06:12:41.696543Z","shell.execute_reply.started":"2023-04-23T06:12:41.688713Z","shell.execute_reply":"2023-04-23T06:12:41.695498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coco_train_dir = '/kaggle/input/mmsample/train2017'\ncoco_valid_dir = '/kaggle/input/mmsample/val2017'\nIMG_COUNT_LIMIT = 5000\n\n# coco_train_dir = '../data/coco/coco-2017-sample/train2017'\n# coco_valid_dir = '../data/coco/coco-2017-sample/val2017'\n# IMG_COUNT_LIMIT = 5000\n\n# coco_train_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n# coco_valid_dir = '/kaggle/input/coco-2017-dataset/coco2017/val2017'\n# IMG_COUNT_LIMIT = 20000","metadata":{"id":"CEcSBOP815en","execution":{"iopub.status.busy":"2023-04-23T06:12:41.698070Z","iopub.execute_input":"2023-04-23T06:12:41.699084Z","iopub.status.idle":"2023-04-23T06:12:41.706636Z","shell.execute_reply.started":"2023-04-23T06:12:41.699012Z","shell.execute_reply":"2023-04-23T06:12:41.705416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')\n\n# coco_train_dir = '/content/drive/MyDrive/mmsample/train2017'\n# coco_valid_dir = '/content/drive/MyDrive/mmsample/val2017'\n# IMG_COUNT_LIMIT = 5000","metadata":{"id":"M76229KOx_n9","outputId":"fca98611-7a89-4e95-ba01-a671a767f4ed","execution":{"iopub.status.busy":"2023-04-23T06:12:41.708429Z","iopub.execute_input":"2023-04-23T06:12:41.709615Z","iopub.status.idle":"2023-04-23T06:12:41.716284Z","shell.execute_reply.started":"2023-04-23T06:12:41.709576Z","shell.execute_reply":"2023-04-23T06:12:41.715406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.Load Data","metadata":{"id":"QrCh-mRzy8G-"}},{"cell_type":"code","source":"class CoCoDataset(Dataset):\n\n    def __init__(self, files: List[str]) -> None:\n        \"\"\" Reads a list of image paths and defines transformations \"\"\"\n        self.files = files\n        self.transformations = Compose([\n            Resize((IMG_WIDTH, IMG_HEIGHT), antialias=False)\n        ])\n\n\n    def __len__(self) -> int:\n        \"\"\" Returns number of images \"\"\"\n        return len(self.files)\n\n\n    def __getitem__(self, i: int):\n        \"\"\" Reads and returns and image \"\"\"\n        img = torchvision.io.read_image(self.files[i])  # Load the image file\n        img = self.transformations(img)                 # Apply transformations\n\n        if img.shape[0] == 1:\n            img = torch.cat([img] * 3)\n\n        return img / 255.0","metadata":{"id":"vDVIWKpJx2DB","execution":{"iopub.status.busy":"2023-04-23T06:12:41.717764Z","iopub.execute_input":"2023-04-23T06:12:41.718482Z","iopub.status.idle":"2023-04-23T06:12:41.728107Z","shell.execute_reply.started":"2023-04-23T06:12:41.718444Z","shell.execute_reply":"2023-04-23T06:12:41.726865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reads image files into a list\ntrain_files = [str(file) for file in Path(coco_train_dir).glob(\"*.jpg\")]\nvalid_files = [str(file) for file in Path(coco_valid_dir).glob(\"*.jpg\")]\n\n# Limit the dataset image counts\ntrain_files = train_files[:IMG_COUNT_LIMIT]\nvalid_files = valid_files[:IMG_COUNT_LIMIT]\n\n# Use custom dataset loader\ntrain_dataset = CoCoDataset(train_files)\nvalid_dataset = CoCoDataset(valid_files)\n\n# Define data loaders\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size = 48, \n    shuffle = True,\n    drop_last = False, \n    num_workers = 2 if torch.cuda.is_available() else 4,\n    pin_memory = True,  # avoid one implicit CPU-to-CPU copy\n)\nvalid_loader = DataLoader(\n    valid_dataset, \n    batch_size = 48, \n    shuffle = True, \n    drop_last = False, \n    num_workers = 2 if torch.cuda.is_available() else 4,\n    pin_memory = True,  # avoid one implicit CPU-to-CPU copy\n)","metadata":{"id":"ZM1OSzhLikXe","execution":{"iopub.status.busy":"2023-04-23T06:12:41.733850Z","iopub.execute_input":"2023-04-23T06:12:41.735001Z","iopub.status.idle":"2023-04-23T06:12:42.270648Z","shell.execute_reply.started":"2023-04-23T06:12:41.734955Z","shell.execute_reply":"2023-04-23T06:12:42.269603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.Model Architecture","metadata":{"id":"2QMR5yayi14t"}},{"cell_type":"markdown","source":"### 2.1.Loss","metadata":{"id":"V_ilGF5e3akU"}},{"cell_type":"code","source":"class VAELoss(nn.Module):\n\n    def __init__(self, kl_alpha: float = 0.01):\n        super(VAELoss, self).__init__()\n        self.kl_alpha = kl_alpha\n\n\n    def _kl(self, mean: float, logvar: float) -> float:\n        \"\"\"\n            Compute the KL divergence between the prior and the approximate posterior\n\n            Parameters:\n                mean (float): mean of the approximate posterior\n                logvar (float): log variance of the approximate posterior\n            Returns:\n                kl (float): KL divergence between the prior and the approximate posterior\n        \"\"\"\n        return (logvar ** 2 + mean ** 2 - torch.log(logvar) - 1/2).sum()\n\n\n    def _mse(self, x: torch.Tensor, x_hat: torch.Tensor) -> float:\n        \"\"\"\n            Computes MSE between the actual and reconstructed images\n            \n            Parameters:\n                x (torch.Tensor): Input tensor\n                x_hat (torch.Tensor): Output tensor\n            Returns:\n                (float) Mean squared error\n        \"\"\"\n        return F.mse_loss(x_hat, x, reduction = 'sum')\n\n\n    def forward(self, x: torch.Tensor, x_hat: torch.Tensor, mean: float, logvar: float) -> float:\n        \"\"\"\n            Calculates the overal loss of the VAE\n            \n            Parameters:\n                x (torch.Tensor): Input tensor\n                x_hat (torch.Tensor): Output tensor\n                mean (float): mean of the approximate posterior\n                logvar (float): log variance of the approximate posterior\n            Returns:\n                 (float) Overal loss\n        \"\"\"\n        mse = self._mse(x, x_hat)\n        kl = self._kl(mean, logvar)\n        loss = mse + kl\n        return loss, kl, mse","metadata":{"id":"R1ZuX1S03cYH","execution":{"iopub.status.busy":"2023-04-23T06:12:42.272384Z","iopub.execute_input":"2023-04-23T06:12:42.273033Z","iopub.status.idle":"2023-04-23T06:12:42.282454Z","shell.execute_reply.started":"2023-04-23T06:12:42.272993Z","shell.execute_reply":"2023-04-23T06:12:42.281412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KL_ALPHA = 10\n\nvae_loss = VAELoss(KL_ALPHA)\n_ = vae_loss.to(DEVICE)","metadata":{"id":"6s3NAKHU3oFN","execution":{"iopub.status.busy":"2023-04-23T06:12:42.284151Z","iopub.execute_input":"2023-04-23T06:12:42.284560Z","iopub.status.idle":"2023-04-23T06:12:42.297708Z","shell.execute_reply.started":"2023-04-23T06:12:42.284521Z","shell.execute_reply":"2023-04-23T06:12:42.296594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.VariationalEncoder","metadata":{"id":"boHdKcoU3c8Y"}},{"cell_type":"code","source":"class VariationalEncoder(nn.Module):\n    def __init__(self, latent_dim: int):\n        super(VariationalEncoder, self).__init__()\n\n        self.model_1 = nn.Sequential(\n            # Convolution layer 1\n            nn.Conv2d(3, 8, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n            # Convolution layer 2\n            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(16),\n            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n            # Convolution layer 3\n            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n            # Convolution layer 4\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n        )\n\n        self.model_2 = nn.Sequential(\n            # Linear layer 1\n            nn.Flatten(),\n            nn.Linear(16384, 64 * 16 * 16),\n            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n        )\n\n        # Mean and logvar layers\n        self.mean_layer = nn.Linear(64 * 16 * 16, latent_dim)\n        self.logvar_layer = nn.Linear(64 * 16 * 16, latent_dim)\n\n        self.N = torch.distributions.Normal(0, 1)\n        if torch.cuda.is_available():\n            # Hack to get sampling on the GPU\n            self.N.loc = self.N.loc.cuda()\n            self.N.scale = self.N.scale.cuda()\n\n        # Metrics\n        self.mean = 0\n        self.logvar = 0\n\n\n    def forward(self, x, is_inference: bool = False):\n        \"\"\"\n            Forward pass of the Encoder\n            \n            Parameters:\n                x (torch.Tensor): input Tensor\n                is_inference (bool): Uses Gaussian sampling in training mode and random sampling when in inference\n        \"\"\"\n        \n        x = x.to(DEVICE)\n        feat = self.model_1(x)\n        x = self.model_2(feat)\n\n        # Calculate mean and standard deviation\n        self.mean =  self.mean_layer(x)\n        self.logvar = torch.exp(self.logvar_layer(x))\n\n        if is_inference:\n            # Random sampling\n            return self.mean + self.logvar * torch.randn(self.mean.shape, device = DEVICE), feat\n        else:\n            # Gaussain sampling\n            return self.mean + self.logvar * self.N.sample(self.mean.shape), feat","metadata":{"id":"2Moy0qt-0Og4","execution":{"iopub.status.busy":"2023-04-23T06:12:42.299212Z","iopub.execute_input":"2023-04-23T06:12:42.299666Z","iopub.status.idle":"2023-04-23T06:12:42.314309Z","shell.execute_reply.started":"2023-04-23T06:12:42.299627Z","shell.execute_reply":"2023-04-23T06:12:42.313215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.Decoder","metadata":{"id":"uXILcYxm3gJM"}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \n    def __init__(self, latent_dim: int):\n        super().__init__()\n\n        self.model_1 = nn.Sequential(\n            # Linear layer 1\n            nn.Linear(latent_dim, 128),\n            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n            # Linear layer 2\n            nn.Linear(128, 64 * 16 * 16),\n            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n            nn.Unflatten(1, (64, 16, 16)),\n        )\n\n        self.model_2 = nn.Sequential(\n            # Convolution layer 1\n            nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n            # Convolution layer 2\n            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(16),\n            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n            # Convolution layer 3\n            nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(8),\n            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n            # Convolution layer 4\n            nn.ConvTranspose2d(8, 3, kernel_size=4, stride=2, padding=1)\n        )\n\n\n    def forward(self, x, feat):\n        x = self.model_1(x)     # Apply linear and convolutional layers\n        x = torch.concat((x, feat), dim=1)\n        x = self.model_2(x)\n        x = torch.sigmoid(x)  # Scale outputs to be in [0, 1]\n        return x","metadata":{"id":"L25QwkCrikdP","execution":{"iopub.status.busy":"2023-04-23T06:12:42.317614Z","iopub.execute_input":"2023-04-23T06:12:42.317887Z","iopub.status.idle":"2023-04-23T06:12:42.331167Z","shell.execute_reply.started":"2023-04-23T06:12:42.317862Z","shell.execute_reply":"2023-04-23T06:12:42.330011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.VariationalAutoencoder","metadata":{"id":"1AC48x1u3jNY"}},{"cell_type":"code","source":"class VariationalAutoencoder(nn.Module):\n    def __init__(self, latent_dim):\n        super(VariationalAutoencoder, self).__init__()\n        self.encoder = VariationalEncoder(latent_dim)\n        self.decoder = Decoder(latent_dim)\n\n    def forward(self, x, is_inference: bool = False):\n        x, feat = self.encoder(x, is_inference)\n        x_hat = self.decoder(x, feat)\n        return x_hat","metadata":{"id":"8LKIXdPEivK9","execution":{"iopub.status.busy":"2023-04-23T06:12:42.332927Z","iopub.execute_input":"2023-04-23T06:12:42.333325Z","iopub.status.idle":"2023-04-23T06:12:42.341987Z","shell.execute_reply.started":"2023-04-23T06:12:42.333285Z","shell.execute_reply":"2023-04-23T06:12:42.340843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LATENT_DIM = 128\n\nvae = VariationalAutoencoder(LATENT_DIM)\n_ = vae.to(DEVICE)\n\n# Summary of the model structure\n# summary(vae, input_size = (IMG_CHANNELS, IMG_WIDTH, IMG_HEIGHT))","metadata":{"id":"GbpTMPEHjAJG","outputId":"5282b59a-31db-4021-9c7e-bff20f2ad0d1","execution":{"iopub.status.busy":"2023-04-23T06:12:42.343376Z","iopub.execute_input":"2023-04-23T06:12:42.343854Z","iopub.status.idle":"2023-04-23T06:12:48.261485Z","shell.execute_reply.started":"2023-04-23T06:12:42.343818Z","shell.execute_reply":"2023-04-23T06:12:48.260407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.Plots and Utilities","metadata":{"id":"caalnNoh3AEl"}},{"cell_type":"code","source":"def plot_metrics(train_loss: List[float], val_loss: List[float]) -> None:\n    \"\"\"\n        Plots the metrics\n        \n        Parameters:\n            train_loss (List[float]): List of train losses for each epoch\n            test_loss (List[float]): List of test losses for each epoch\n        Returns:\n            None\n    \"\"\"\n    plt.figure(figsize = (20, 5))\n    plt.title(\"Training and Validation Loss\")\n    plt.plot(train_loss, label = \"training\")\n    plt.plot(val_loss, label = \"validation\")\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Loss\")\n    plt.xticks(range(len(train_loss)))\n    plt.legend()\n    plt.show()","metadata":{"id":"ZnRNDPOFw7t6","execution":{"iopub.status.busy":"2023-04-23T06:12:48.265071Z","iopub.execute_input":"2023-04-23T06:12:48.267150Z","iopub.status.idle":"2023-04-23T06:12:48.274203Z","shell.execute_reply.started":"2023-04-23T06:12:48.267116Z","shell.execute_reply":"2023-04-23T06:12:48.273240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_reconstruction(model: VariationalAutoencoder, dataset: CoCoDataset, n: int = 5) -> None:\n    \"\"\"\n        Plot the original and reconstructed images\n\n        Parameters:\n            model (VAE): The VAE models\n            dataset (torch.utils.data.Dataset): Dataset to use samples from\n            n (int): Number of images to plot\n        Returns:\n            None\n    \"\"\"\n\n    plt.figure(figsize = (10, 3))\n\n    for i in range(n):\n        ax = plt.subplot(2, n, i + 1)\n        img = dataset[i].unsqueeze(0).to(DEVICE)\n        model.encoder.eval()\n        model.decoder.eval()\n\n        # Reconstruct the image using the encoder and decoder\n        with torch.no_grad():\n            rec_img = vae(img)\n\n        # Plot original images\n        plt.imshow(img.cpu().squeeze().permute(1, 2, 0).numpy())\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)  \n        if i == n // 2:\n            ax.set_title('Original images')\n\n        # Plot reconstructed images\n        ax = plt.subplot(2, n, i + 1 + n)\n        plt.imshow(rec_img.cpu().squeeze().permute(1, 2, 0).numpy())  \n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)  \n        if i == n // 2:\n            ax.set_title('Reconstructed images')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"yJLo-JNU3C0t","execution":{"iopub.status.busy":"2023-04-23T06:12:48.275559Z","iopub.execute_input":"2023-04-23T06:12:48.275941Z","iopub.status.idle":"2023-04-23T06:12:48.287753Z","shell.execute_reply.started":"2023-04-23T06:12:48.275893Z","shell.execute_reply":"2023-04-23T06:12:48.286771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_psnr(img1: np.array, img2: np.array, border: int = 0):\n    \"\"\"\n        Peak signal-to-noise ratio\n        \n        Parameters:\n            img1 (np.array): First iamge\n            img2 (np.array): Second iamge\n            border (int): Border width of the images\n        Returns:\n            (float) the PSNR score\n    \"\"\"\n\n    if not img1.shape == img2.shape:\n        raise ValueError('Input images must have the same dimensions.')\n\n    # Remove border\n    h, w = img1.shape[:2]\n    img1 = img1[border:h-border, border:w-border]\n    img2 = img2[border:h-border, border:w-border]\n\n    # Calculate MSE\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    mse = np.mean((img1 - img2) ** 2)\n\n    # Calculate PSNR\n    if mse == 0:\n        return float('inf')\n    return 20 * math.log10(255.0 / math.sqrt(mse))","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:12:48.289178Z","iopub.execute_input":"2023-04-23T06:12:48.289776Z","iopub.status.idle":"2023-04-23T06:12:48.304316Z","shell.execute_reply.started":"2023-04-23T06:12:48.289734Z","shell.execute_reply":"2023-04-23T06:12:48.303206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.Training & Testing","metadata":{"id":"TjdRHJYJjWPS"}},{"cell_type":"code","source":"def train_epoch(model: VariationalAutoencoder, dataloader: DataLoader, optimizer: optim) -> float:\n    \"\"\"\n        Trains the model for one epoch\n\n        Parameters:\n            model (VAE): The model to train\n            dataloader (DataLoader): Dataloader to use for training\n            optimizer (torch.optim): Optimizer to use for training\n        Returns:\n            train_loss (float): Average training loss for the epoch\n    \"\"\"\n    print(\"training\", end = \"\\t\")\n\n    # Set training mode for encoder and decoder\n    model.train()\n    loss = 0.0\n\n    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n    for _, x in enumerate(dataloader):\n        # load tensor to device\n        x = x.to(DEVICE)\n        # Run input through model\n        x_hat = model(x)\n        # Calculate batch loss (train_loss)\n        _loss, kl, mse = vae_loss(x, x_hat, model.encoder.mean, model.encoder.logvar)\n        # Backward pass\n        optimizer.zero_grad()\n        _loss.backward()\n        # Gradient Clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n        optimizer.step()\n        # Cumulate loss\n        loss += _loss.item()\n\n        print(\"=\", end = \"\")\n\n    print(\">\", end = \"\\t\")\n    return loss / len(dataloader.dataset)","metadata":{"id":"tsPdAbEf3Ktb","execution":{"iopub.status.busy":"2023-04-23T06:12:48.305783Z","iopub.execute_input":"2023-04-23T06:12:48.306311Z","iopub.status.idle":"2023-04-23T06:12:48.319502Z","shell.execute_reply.started":"2023-04-23T06:12:48.306268Z","shell.execute_reply":"2023-04-23T06:12:48.318598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_epoch(vae: VariationalAutoencoder, dataloader: DataLoader) -> float:\n    \"\"\"\n        Test the model for one epoch\n\n        Parameters:\n            model (VAE): The model to test\n            dataloader (DataLoader): Dataloader to use for testing\n            verbose (bool): Whether to print the loss for each batch\n        Returns:\n            val_loss (float): Average validation loss for the epoch\n    \"\"\"\n    print(\"validating\", end = \"\\t\")\n\n    # Set evaluation mode for encoder and decoder\n    vae.eval()\n    loss = 0.0\n\n    with torch.no_grad(): # No need to track the gradients\n        for _, x in enumerate(dataloader):\n            # load tensor to device\n            x = x.to(DEVICE)\n            # Run input through model\n            x_hat = vae(x)\n            # Calculate batch loss (test_loss)\n            _loss, kl, mse = vae_loss(x, x_hat, vae.encoder.mean, vae.encoder.logvar)\n            loss += _loss.item()\n\n            print(\"=\", end = \"\")\n\n    print(\">\", end = \"\\t\")\n    return loss / len(dataloader.dataset)","metadata":{"id":"aSubrz0i4AKu","execution":{"iopub.status.busy":"2023-04-23T06:12:48.322669Z","iopub.execute_input":"2023-04-23T06:12:48.322937Z","iopub.status.idle":"2023-04-23T06:12:48.335010Z","shell.execute_reply.started":"2023-04-23T06:12:48.322912Z","shell.execute_reply":"2023-04-23T06:12:48.334040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(vae: VariationalAutoencoder, train_loader: DataLoader, valid_loader: DataLoader, epochs: int, optimizer: optim) -> Tuple[list, list]:\n    \"\"\"\n        Train the model\n\n        Parameters:\n            vae (VariationalAutoencoder): VAE to train\n            train_loader (DataLoader): Dataloader to use for training\n            valid_loader (DataLoader): Dataloader to use for validation\n            epochs (int): Number of epochs to train for\n            optimizer (torch.optim): Optimizer to use for training\n        Returns:\n            None\n    \"\"\"\n\n    train_loss = []\n    val_loss = []\n\n    for epoch in range(epochs):\n        # Train\n        print(f\"EPOCH {epoch + 1})\", end = \"\\t\")\n        _train_loss = train_epoch(vae, train_loader, optimizer)\n        print(f\"loss: {round(_train_loss, 1)}\")\n        train_loss.append(_train_loss)\n\n        # Test\n        print(f\"EPOCH {epoch + 1})\", end = \"\\t\")\n        _val_loss = test_epoch(vae, valid_loader)\n        print(f\"loss: {round(_val_loss, 1)}\")\n        val_loss.append(_val_loss)\n\n        plot_reconstruction(vae, valid_dataset, n = 10)\n        print(\"\")\n    return train_loss, val_loss","metadata":{"id":"aZQdHI145R4B","execution":{"iopub.status.busy":"2023-04-23T06:12:48.336767Z","iopub.execute_input":"2023-04-23T06:12:48.337612Z","iopub.status.idle":"2023-04-23T06:12:48.349519Z","shell.execute_reply.started":"2023-04-23T06:12:48.337574Z","shell.execute_reply":"2023-04-23T06:12:48.348513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 12\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\n\n# Define optimizaer\noptimizer = torch.optim.Adam(\n    vae.parameters(),\n    lr = LEARNING_RATE,\n    weight_decay = WEIGHT_DECAY\n)\n# Train model\ntrain_loss, val_loss = train(vae, train_loader, valid_loader, EPOCHS, optimizer)","metadata":{"id":"lUB0OHsK0P5N","outputId":"83bf9e25-7cee-4206-dcb8-098834fd6bd0","execution":{"iopub.status.busy":"2023-04-23T06:12:48.352256Z","iopub.execute_input":"2023-04-23T06:12:48.352982Z","iopub.status.idle":"2023-04-23T06:28:27.915597Z","shell.execute_reply.started":"2023-04-23T06:12:48.352945Z","shell.execute_reply":"2023-04-23T06:28:27.914434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metrics(train_loss, val_loss)","metadata":{"id":"KWrSmkidEJ39","execution":{"iopub.status.busy":"2023-04-23T06:28:27.917282Z","iopub.execute_input":"2023-04-23T06:28:27.918175Z","iopub.status.idle":"2023-04-23T06:28:28.187307Z","shell.execute_reply.started":"2023-04-23T06:28:27.918125Z","shell.execute_reply":"2023-04-23T06:28:28.186290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.Saving/Loading The Model","metadata":{}},{"cell_type":"code","source":"# Save\n# torch.save(vae.state_dict(), vae)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:28:28.188841Z","iopub.execute_input":"2023-04-23T06:28:28.189172Z","iopub.status.idle":"2023-04-23T06:28:28.196300Z","shell.execute_reply.started":"2023-04-23T06:28:28.189135Z","shell.execute_reply":"2023-04-23T06:28:28.192603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load\n# vae.load_state_dict(torch.load(vae))","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:28:28.197826Z","iopub.execute_input":"2023-04-23T06:28:28.198307Z","iopub.status.idle":"2023-04-23T06:28:28.205184Z","shell.execute_reply.started":"2023-04-23T06:28:28.198252Z","shell.execute_reply":"2023-04-23T06:28:28.204013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.Inference","metadata":{}},{"cell_type":"code","source":"def gauss_noise_tensor(img):\n    assert isinstance(img, torch.Tensor)\n    dtype = img.dtype\n    if not img.is_floating_point():\n        img = img.to(torch.float32)\n    \n    sigma = 25.0\n    \n    out = img + sigma * torch.randn_like(img)\n    \n    if out.dtype != dtype:\n        out = out.to(dtype)\n        \n    return out","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:28:28.206597Z","iopub.execute_input":"2023-04-23T06:28:28.206937Z","iopub.status.idle":"2023-04-23T06:28:28.215140Z","shell.execute_reply.started":"2023-04-23T06:28:28.206900Z","shell.execute_reply":"2023-04-23T06:28:28.214421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_random_reconstructions(model: VariationalAutoencoder, dataset: CoCoDataset, n: int = 3, times: int = 5) -> None:\n    \"\"\"\n        Plot the original and randomly reconstructed images\n\n        Parameters:\n            model (VAE): The VAE models\n            dataset (torch.utils.data.Dataset): Dataset to use samples from\n            n (int): Number of images to plot\n            times (int): Number of times to feed an image to the network\n        Returns:\n            None\n    \"\"\"\n\n    vae.encoder.eval()\n    vae.decoder.eval()\n\n    for i in range(n):\n        ax = plt.subplot(n, times + 1, (i * (times + 1) + 1))\n        img = dataset[i].unsqueeze(0).to(DEVICE)\n\n        # Plot the original image\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)  \n        plt.imshow(img.cpu().squeeze().permute(1, 2, 0).numpy())\n        ax.set_title('Original')\n\n        H_img = img.cpu().squeeze().permute(1, 2, 0).numpy()\n\n        for j in range(times):\n\n            # Reconstruct the image using the encoder and decoder\n            with torch.no_grad():\n                noise = torch.randn_like(img) * 0 # Add noise\n                rec_img = vae(img + noise, is_inference = True)\n\n            # Calculate PSNR\n            E_img = rec_img.cpu().squeeze().permute(1, 2, 0).numpy()\n            psnr = calculate_psnr(E_img, H_img, border=0)\n\n            # Plot the reconstructed images\n            ax = plt.subplot(n, times+1, (i * (times + 1)) + j + 2)\n            ax.get_xaxis().set_visible(False)\n            ax.get_yaxis().set_visible(False)  \n            plt.imshow(rec_img.cpu().squeeze().permute(1, 2, 0).numpy())\n            ax.set_title(f'{psnr:0.2f}')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:28:28.220502Z","iopub.execute_input":"2023-04-23T06:28:28.221051Z","iopub.status.idle":"2023-04-23T06:28:28.231990Z","shell.execute_reply.started":"2023-04-23T06:28:28.221013Z","shell.execute_reply":"2023-04-23T06:28:28.231099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_random_reconstructions(vae, valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:28:28.233463Z","iopub.execute_input":"2023-04-23T06:28:28.234182Z","iopub.status.idle":"2023-04-23T06:28:29.246849Z","shell.execute_reply.started":"2023-04-23T06:28:28.234142Z","shell.execute_reply":"2023-04-23T06:28:29.245703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}